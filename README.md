# OrchestrationBench

A comprehensive bilingual (Korean/English) benchmark framework for evaluating Large Language Model (LLM) orchestration capabilities in multi-domain scenarios. This benchmark evaluates how well LLMs can plan complex workflows and execute tools under realistic constraints across 17 representative domains with nearly 100 realistic virtual tools.

## ğŸ¯ Overview

The OrchestrationBench tests the ability of language models to:
- **Workflow-based Planning**: Structure multi-step processes and coordinate across multiple domains using DAG-based evaluation
- **Constraint-aware Tool Execution**: Generate correct function calls with appropriate arguments while handling business constraints and rejection cases

## ğŸ“ Project Structure

```
OrchestrationBench/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ base_config/
â”‚   â”‚   â”œâ”€â”€ eval_config.yaml       # Default judge model and evaluation settings
â”‚   â”‚   â””â”€â”€ multiagent_config.yaml # LLM model configurations and prompts
â”‚   â”œâ”€â”€ bedrock.yaml               # AWS Bedrock provider config
â”‚   â”œâ”€â”€ claude.yaml                # Anthropic Claude provider config
â”‚   â”œâ”€â”€ gemini.yaml                # Google Gemini provider config
â”‚   â”œâ”€â”€ openai.yaml                # OpenAI provider config
â”‚   â”œâ”€â”€ opensource_api.yaml        # External vLLM server config
â”‚   â””â”€â”€ opensource_vllm.yaml       # Local vLLM server config
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ EN/                        # English scenarios and agent cards
â”‚   â”‚   â”œâ”€â”€ multiagent_cards/      # 17 domain agent definitions (17 agents)
â”‚   â”‚   â””â”€â”€ scenario_data/         # Test scenarios (222 YAML files)
â”‚   â”œâ”€â”€ KO/                        # Korean scenarios and agent cards
â”‚   â”‚   â”œâ”€â”€ multiagent_cards/      # 17 domain agent definitions (17 agents)
â”‚   â”‚   â””â”€â”€ scenario_data/         # Test scenarios (222 YAML files, Korean version)
â”‚   â””â”€â”€ results/                   # Generated results and evaluation outputs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ stepwise_scenario_processor.py  # Main scenario execution engine
â”‚   â”œâ”€â”€ evaluation.py              # Comprehensive evaluation pipeline
â”‚   â”œâ”€â”€ orchestration_engine.py    # Simplified orchestration engine
â”‚   â”œâ”€â”€ step_history_generator.py  # History generation logic
â”‚   â”œâ”€â”€ agents/                    # Agent implementations
â”‚   â”œâ”€â”€ models/                    # LLM model implementations
â”‚   â””â”€â”€ utils/                     # Utility modules
â”œâ”€â”€ .env.example                  # Example environment variables
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This documentation
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- UV package manager (recommended) or pip
- API keys for the LLMs you want to test

### Installation

1. **Clone the repository:**
```bash
git clone <repository-url>
cd OrchestrationBench
```

2. **Install dependencies:**
```bash
uv sync
# or with pip: pip install -e .
# Note: scipy is required for workflow evaluation
```

3. **Set up environment variables:**
```bash
cp .env.example .env
# Edit .env with your API keys
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
# Add other API keys as needed
```

### Basic Usage: Full Pipeline with `uv run evaluate`

The simplest way to run the full evaluation pipeline (scenario processing + evaluation) is using the `evaluate` CLI command with a YAML configuration file.

```bash
# Run evaluation with OpenAI models
uv run evaluate config/openai.yaml

# Run evaluation with Claude models
uv run evaluate config/claude.yaml

# Run evaluation with custom experiment ID
uv run evaluate config/openai.yaml --experiment-id my_experiment

# Run evaluation with custom output directory and verbose logging
uv run evaluate config/claude.yaml --results-dir ./my_results --verbose
```

**CLI Options:**
| Option | Short | Description |
|--------|-------|-------------|
| `config_path` | - | Path to the configuration YAML file (required) |
| `--experiment-id` | `-e` | Custom experiment ID. Defaults to timestamp if not provided |
| `--results-dir` | `-o` | Directory to save results. Defaults to `./results` |
| `--verbose` | `-v` | Enable verbose (DEBUG level) logging |

**Example Configuration Files:**
- `config/openai.yaml` - OpenAI models (GPT-4, GPT-4-mini, etc.)
- `config/claude.yaml` - Anthropic Claude models
- `config/gemini.yaml` - Google Gemini models
- `config/bedrock.yaml` - AWS Bedrock models
- `config/opensource_api.yaml` - Open-source models via external vLLM server
- `config/opensource_vllm.yaml` - Open-source models with auto-started vLLM

See the [Configuration](#ï¸-configuration) section for details on creating custom configuration files.

---

### Advanced Usage: Running Individual Steps

For more granular control, you can run the scenario processing and evaluation steps separately.

#### Step 1: Scenario Processing (Data Generation)

Generate LLM interaction histories by running scenarios against the target model:

```bash
# Process all English scenarios with GPT-4
uv run python src/stepwise_scenario_processor.py \
  --model gpt-4.1-mini \
  --agent-cards data/EN/multiagent_cards \
  --data-path "data/EN/scenario_data/*.yaml" \
  --num-iter 3 \
  --output-dir data/results/step_wise_evaluation

# Process a specific scenario by ID
uv run python src/stepwise_scenario_processor.py \
  --model gpt-4.1-mini \
  --agent-cards data/EN/multiagent_cards \
  --data-path "data/EN/scenario_data/*.yaml" \
  --scenario-id 1 \
  --num-iter 3 \
  --output-dir data/results/step_wise_evaluation_TEST

# Process Korean scenarios
uv run python src/stepwise_scenario_processor.py \
  --model gpt-4.1-mini \
  --agent-cards data/KO/multiagent_cards \
  --data-path "data/KO/scenario_data/*.yaml" \
  --num-iter 3 \
  --output-dir data/results/step_wise_evaluation_KO
```

**Scenario Processor Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `--model` | Model name defined in `config/base_config/multiagent_config.yaml` | Required |
| `--agent-cards` | Directory containing agent card JSON files (EN or KO) | Required |
| `--data-path` | Path pattern to scenario YAML files (supports glob patterns) | Required |
| `--scenario-id` | Process only a specific scenario by its ID | All scenarios |
| `--output-dir` | Directory where results will be saved | Required |
| `--max-scenarios` | Limit number of scenarios to process | Unlimited |
| `--num-iter` | Number of iterations per scenario | 1 |
| `--batch-size` | Number of concurrent agent executions | 5 |
| `--max-retries` | Maximum retry attempts for failed executions | 10 |

#### Step 2: Evaluation

Evaluate the generated results using the evaluation script:

```bash
# Evaluate results from a specific model
uv run python src/evaluation.py \
  --input data/results/step_wise_evaluation/gpt-4.1-mini \
  --agent-cards-path data/EN/multiagent_cards \
  --eval-config config/base_config/eval_config.yaml \
  --output evaluation_output.json \
  --sequential \
  --log-level INFO

# Evaluate with detailed LLM evaluation logs
uv run python src/evaluation.py \
  --input data/results/step_wise_evaluation/gpt-4.1-mini \
  --agent-cards-path data/EN/multiagent_cards \
  --save-llm-results \
  --llm-results-dir evaluation_logs

# Quick evaluation without LLM-based argument checking
uv run python src/evaluation.py \
  --input data/results/step_wise_evaluation/gpt-4.1-mini \
  --agent-cards-path data/EN/multiagent_cards \
  --skip-llm-eval
```

**Evaluation Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `--input` | Path to input file or directory containing results | Required |
| `--agent-cards-path` | Directory containing agent card JSON files | Required |
| `--eval-config` | Path to evaluation configuration file | `config/base_config/eval_config.yaml` |
| `--output` | Output JSON file path | Auto-generated |
| `--sequential` | Process files one by one (recommended for stability) | False |
| `--skip-llm-eval` | Skip LLM-based argument evaluation (faster) | False |
| `--save-llm-results` | Save detailed LLM evaluation logs | False |
| `--llm-results-dir` | Directory for LLM evaluation logs | `llm_evaluation_logs` |
| `--start-over` | Delete temporary files and restart evaluation | False |
| `--pattern` | File pattern to match in directory mode | `*_out.json` |
| `--log-level` | Logging verbosity (DEBUG, INFO, WARNING, ERROR) | INFO |

## âš™ï¸ Configuration

### Evaluation Pipeline Configuration

The `uv run evaluate` command uses YAML configuration files with Hydra-style defaults. Config files inherit from `base_config/eval_config.yaml` and can override any settings.

> **Note:** For detailed configuration options and examples, refer to the individual config files in the `config/` directory (e.g., `openai.yaml`, `claude.yaml`, `gemini.yaml`, `bedrock.yaml`, `opensource_api.yaml`, `opensource_vllm.yaml`).

```yaml
# Hydra-style defaults (inherits judge model settings from base_config)
defaults:
  - base_config/eval_config
  - _self_

# Model configuration (target model to evaluate)
model:
  provider: openai          # Provider: openai, claude, gemini, bedrock, opensource
  model_alias: gpt-4.1-mini # Display name for results
  model: gpt-4.1-mini       # Actual model identifier
  base_url: https://api.openai.com/v1
  api_key: ${OPENAI_API_KEY} # Environment variable substitution
  temperature: 0.2
  # For opensource provider with local model:
  # model_path: /path/to/your/model

# Benchmark execution configuration
benchmark:
  temperature: 0.2          # Temperature for generation
  num_iter: 3               # Number of iterations per scenario
  batch_size: 50            # Concurrent executions
  max_retries: 10           # Retry attempts for failures

  # Optional: HuggingFace Hub upload
  # hf_hug_log_args:
  #   hub_results_org: your-org
  #   hub_repo_name: orchestration-bench-results
  #   push_results_to_hub: true
  #   public_repo: false

# Optional: Override judge model (defaults from base_config/eval_config)
# judge:
#   model:
#     provider: claude
#     model: claude-haiku-4-5-20251001
#     base_url: https://api.anthropic.com
#     api_key: ${ANTHROPIC_API_KEY}
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0

# Optional: vLLM server configuration (only for opensource provider with model_path)
# vllm:
#   port: 15142
#   tensor_parallel_size: 1
#   gpu_memory_utilization: 0.95
#   max_model_len: null
#   reasoning_parser: null
#   tool_call_parser: null
#   extra_args:
#     - --trust-remote-code
```

**Supported Providers:**
| Provider | Description | Required Fields |
|----------|-------------|-----------------|
| `openai` | OpenAI API | `model`, `api_key`, `base_url` |
| `claude` | Anthropic Claude | `model`, `api_key`, `base_url` |
| `gemini` | Google Gemini | `model`, `api_key` |
| `bedrock` | AWS Bedrock | `model`, `aws_access_key_id`, `aws_secret_access_key`, `aws_region` (optional, default: `us-east-1`) |
| `opensource` | vLLM-served models | `model`, `model_url` or `model_path` |

**vLLM Configuration Options:**
| Option | Description | Default |
|--------|-------------|---------|
| `port` | Port number for vLLM server | 15142 |
| `tensor_parallel_size` | Number of GPUs for tensor parallelism | 1 |
| `gpu_memory_utilization` | Fraction of GPU memory to use | 0.95 |
| `max_model_len` | Maximum model context length | Auto-detected |
| `reasoning_parser` | Parser for reasoning models | null |
| `tool_call_parser` | Parser for tool call outputs | null |
| `extra_args` | Additional vLLM command-line arguments | [] |

---

### Judge Model Configuration (`config/base_config/eval_config.yaml`)

Configure the judge model used for evaluation. This is the default configuration that can be overridden in provider-specific config files:

```yaml
# Judge model settings (used for evaluating model outputs)
judge:
  model:
    provider: openai
    base_url: https://api.openai.com/v1
    model: gpt-4.1
    api_key: ${OPENAI_API_KEY}
  generation_params:
    temperature: 0.3
    max_tokens: 12288
    top_p: 1.0

evaluation_params:
  agent_change_weight: 0.8
  status_change_weight: 0.2
```

**Overriding the Judge Model:**

You can override the judge model in any provider config file:

```yaml
# config/bedrock.yaml
defaults:
  - base_config/eval_config
  - _self_

model:
  provider: bedrock
  model: claude-sonnet-4
  ...

# Use Claude as judge instead of default
judge:
  model:
    provider: claude
    model: claude-haiku-4-5-20251001
    base_url: https://api.anthropic.com
    api_key: ${ANTHROPIC_API_KEY}
```

### Agent Cards

The system includes 17 specialized agents, each defined in JSON format:

#### English Agents (`data/EN/multiagent_cards/`)
- `calendar_agent.json` - Calendar and scheduling operations
- `weather_agent.json` - Weather information retrieval
- `search_agent.json` - Web search and information gathering  
- `place_agent.json` - Location and place information
- `transport_agent.json` - Transportation and routing
- `finance_agent.json` - Financial information and transactions
- `shopping_agent.json` - E-commerce and shopping tasks
- `entertainment_agent.json` - Movies, shows, and entertainment
- `travel_agent.json` - Travel planning and booking
- `message_agent.json` - Communication and messaging
- `news_agent.json` - News and current events
- `person_agent.json` - People and celebrity information
- `counsel_agent.json` - Counseling and advice
- `delivery_agent.json` - Package delivery and logistics
- `life_info_agent.json` - Daily life information
- `personal_banking_agent.json` - Personal banking operations
- `sports_agent.json` - Sports information and scores

#### Korean Agents (`data/KO/multiagent_cards/`)
Similar set of agents with Korean language support and localized capabilities.

## ğŸ“ˆ Results and Analysis

Results are saved in structured JSON format with comprehensive metrics for both workflow planning and tool execution evaluation.

### Key Metrics (Top-Level Summary)

| Metric | Formula | Description |
|--------|---------|-------------|
| **Average** | `(Call Rejection + FC + Plan) / 3` | Overall performance score |
| **Call Rejection Classification Accuracy** | `(reject_f1 + fc_f1) / 2` | Rejection/FC decision F1 average |
| **FC (Function Call)** | `(fn_name_f1 + key_f1 + value_f1) / 3` | Function call quality score |
| **Plan (Workflow)** | `workflow_evaluation_with_failure` | DAG-based planning score |

---

### Call Rejection Classification Accuracy

Measures the model's ability to decide **WHEN** to call tools vs **WHEN** to reject.

**Prediction types:**
- `FC`: Model makes a function call
- `Reject`: Model correctly decides not to call (with rejection type)
- `FAILED`: Model fails to generate a valid prediction

**Rejection cases:**
- `AWAITING_USER_INPUT`: Model should ask for more information
- `TOOL_CONSTRAINT_VIOLATION`: Model should recognize constraints prevent execution

```
Call Rejection = (reject_f1 + fc_f1) / 2
```

**Additional metrics:**
- `rejection_type_accuracy`: When model correctly rejects, how often is the rejection type correct?
- `total_rejection_type_mismatch`: Count of correct rejections with wrong type
- `total_failed_generation`: Count of failed predictions (tracked separately)

---

### FC (Function Call) Score

Measures function call quality when the model decides to call a tool.

```
FC = (function_name_f1 + arguments_key_f1 + arguments_value_f1) / 3

where:
  function_name_f1:   F1 score for correct tool/function name selection
  arguments_key_f1:   F1 score for correct parameter key matching
  arguments_value_f1: F1 score for correct parameter value matching
```

---

For detailed result format, see [`docs/result_format.md`](docs/result_format.md).

## ğŸ¤ Contributing

Please submit issues or pull requests if you find problems with the benchmark or have suggestions for improvements.

## ğŸ“ Contact

For questions or support, please open an issue on the repository.

## ğŸ“š Citation

If you use this benchmark in your research, please cite:

```bibtex
@misc{OrchestrationBench2025,
  title={OrchestrationBench: LLM-driven Agentic Planning and Tool Use in Multi-Domain Scenarios},
  year={2025},
  url={[Repository URL]}
}
```

## License

This software is licensed under the Apache 2 license, quoted below.

Copyright 2025 Kakao Corp. http://www.kakaocorp.com

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.