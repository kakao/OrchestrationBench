# Gemini (Google) Configuration for OrchestrationBench
# Usage: uv run evaluate config/gemini.yaml

defaults:
  - base_config/eval_config
  - _self_

model:
  provider: gemini
  # model_alias: Short name for top-level results directory (e.g., "gemini-2.5-flash")
  # model: Actual API model identifier used for calls (e.g., "gemini-2.5-flash-preview-05-20")
  # If model_alias is omitted, it defaults to model value.
  model_alias: gemini-2.5-flash
  model: gemini-2.5-flash-preview-05-20
  base_url: https://generativeai.googleapis.com
  api_key: ${GEMINI_API_KEY}
  temperature: 0.2

benchmark:
  temperature: 0.2
  num_iter: 3
  batch_size: 50
  max_retries: 10

  # Optional: HuggingFace Hub upload settings
  # hf_hug_log_args:
  #   hub_results_org: your-org
  #   hub_repo_name: orchestration-bench-results
  #   push_results_to_hub: true
  #   public_repo: false

# Optional: Override judge model - defaults from base_config/eval_config
# Uncomment ONE of the following judge configurations:

# --- Judge: OpenAI ---
# judge:
#   model:
#     provider: openai
#     model: gpt-4.1
#     base_url: https://api.openai.com/v1
#     api_key: ${OPENAI_API_KEY}
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0

# --- Judge: Claude (Anthropic API) ---
# judge:
#   model:
#     provider: claude
#     model: claude-haiku-4-5-20251001
#     base_url: https://api.anthropic.com
#     api_key: ${ANTHROPIC_API_KEY}
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0

# --- Judge: Gemini ---
# judge:
#   model:
#     provider: gemini
#     model: gemini-2.5-flash-preview-05-20
#     api_key: ${GEMINI_API_KEY}
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0

# --- Judge: AWS Bedrock ---
# judge:
#   model:
#     provider: bedrock
#     model: claude-sonnet-4
#     aws_access_key_id: ${AWS_ACCESS_KEY_ID}
#     aws_secret_access_key: ${AWS_SECRET_ACCESS_KEY}
#     aws_region: us-east-2
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0

# --- Judge: Opensource (Local vLLM or compatible API) ---
# NOTE: Requires vLLM server to be running beforehand (not auto-started)
# judge:
#   model:
#     provider: opensource
#     model: my-judge-model
#     base_url: http://localhost:8000/v1
#     api_key: EMPTY
#   generation_params:
#     temperature: 0.3
#     max_tokens: 12288
#     top_p: 1.0
